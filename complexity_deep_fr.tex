\documentclass[10pt]{article}
\usepackage{tmlr}
% Si accept\'{e}, utiliser la ligne suivante pour la version camera-ready :
%\usepackage[accepted]{tmlr}

\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{amssymb}
\usepackage{amsthm}

% Environnements de th\'{e}or\`{e}mes
\newtheorem{theorem}{Th\'{e}or\`{e}me}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollaire}
\newtheorem{lemma}[theorem]{Lemme}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{D\'{e}finition}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remarque}

\title{COMPLEXITY-DEEP : Une architecture de mod\`{e}le de langage avec attention guid\'{e}e par Mu et MLP \`{a} routage par token}

\author{\name Auteurs anonymes \\
      \addr Article en cours de r\'{e}vision en double aveugle}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{MM}
\def\year{YYYY}
\def\openreview{\url{https://openreview.net/forum?id=XXXX}}


\begin{document}

\maketitle

\begin{abstract}
Nous pr\'{e}sentons COMPLEXITY-DEEP, une architecture de mod\`{e}le de langage (LLM) d\'{e}velopp\'{e}e from scratch, introduisant trois contributions originales : (1) \textbf{Token-Routed MLP}, un m\'{e}canisme de routage dynamique par token inspir\'{e} des Mixture of Experts mais sans n\'{e}cessiter de perte auxiliaire d'\'{e}quilibrage de charge, (2) \textbf{Mu-Guided Attention}, o\`{u} un \'{e}tat latent $\mu$ de la couche pr\'{e}c\'{e}dente guide les projections K, Q et V, cr\'{e}ant un flux d'information bidirectionnel entre l'attention et la dynamique, et (3) un \textbf{contr\^{o}leur adaptatif de style PiD} qui stabilise l'entra\^{i}nement par un ajustement dynamique de l'\'{e}chelle. Nous fournissons une analyse th\'{e}orique formelle prouvant l'\'{e}quilibre parfait de charge, l'\'{e}quivalence de capacit\'{e} avec les mod\`{e}les denses \`{a} un co\^{u}t de calcul de $1/n$, l'orthogonalisation des experts guid\'{e}e par le gradient, et \'{e}tablissons des connexions entre le guidage Mu et la th\'{e}orie du codage pr\'{e}dictif. Notre impl\'{e}mentation \`{a} 1,5 milliard de param\`{e}tres, entra\^{i}n\'{e}e sur 33 milliards de tokens issus de FineWeb-Edu, d\'{e}montre la viabilit\'{e} de cette architecture avec une convergence stable (perte 3,78, perplexit\'{e} 43,7). L'\'{e}valuation sur des benchmarks standards montre des performances coh\'{e}rentes avec la taille du mod\`{e}le, le fine-tuning supervis\'{e} atteignant 30\% sur MMLU (+5\% au-dessus du hasard) et 23\% sur ARC-Challenge.
\end{abstract}

%=============================================================================
\section{Introduction}
\label{sec:introduction}
%=============================================================================

Les grands mod\`{e}les de langage (LLM) ont r\'{e}volutionn\'{e} le traitement du langage naturel ces derni\`{e}res ann\'{e}es \citep{vaswani2017attention, brown2020language}. Cependant, les architectures dominantes pr\'{e}sentent plusieurs limitations :

\begin{itemize}
    \item \textbf{Flux d'information unidirectionnel} : dans les Transformers standards, l'information ne circule que de bas en haut, sans guidage descendant depuis les couches sup\'{e}rieures.
    \item \textbf{Co\^{u}t computationnel des MoE} : les architectures Mixture of Experts \citep{shazeer2017outrageously, fedus2022switch} n\'{e}cessitent des m\'{e}canismes complexes d'\'{e}quilibrage de charge et de routage.
    \item \textbf{Instabilit\'{e} de l'entra\^{i}nement} : les grands mod\`{e}les souffrent souvent d'instabilit\'{e}s num\'{e}riques n\'{e}cessitant un r\'{e}glage minutieux des hyperparaml\`{e}tres.
\end{itemize}

Nous proposons COMPLEXITY-DEEP, une architecture qui r\'{e}pond \`{a} ces limitations \`{a} travers trois innovations :

\begin{enumerate}
    \item \textbf{Token-Routed MLP} : un routage d\'{e}terministe par token (token\_id modulo nombre d'experts) qui s\'{e}lectionne un expert par token sans perte auxiliaire.
    \item \textbf{Mu-Guided Attention} : un m\'{e}canisme o\`{u} l'\'{e}tat latent $\mu$ de la couche pr\'{e}c\'{e}dente influence les projections K, Q et V, cr\'{e}ant un flux bidirectionnel.
    \item \textbf{Dynamic Scaler de style PiD} : un contr\^{o}leur adaptatif inspir\'{e} des syst\`{e}mes de commande automatique pour stabiliser l'entra\^{i}nement.
\end{enumerate}

%=============================================================================
\section{Travaux connexes}
\label{sec:related_work}
%=============================================================================

\subsection{Architectures Transformer}

L'architecture Transformer \citep{vaswani2017attention} est devenue le standard pour les mod\`{e}les de langage. Les d\'{e}veloppements r\'{e}cents incluent les optimisations de l'attention comme Flash Attention \citep{dao2022flashattention}, Grouped Query Attention (GQA) \citep{ainslie2023gqa} et Rotary Position Embeddings (RoPE) \citep{su2021roformer}.

\subsection{Mixture of Experts}

Les architectures MoE \citep{shazeer2017outrageously} permettent d'augmenter la capacit\'{e} du mod\`{e}le sans augmenter proportionnellement le co\^{u}t computationnel. Switch Transformer \citep{fedus2022switch} et Mixtral \citep{jiang2024mixtral} ont d\'{e}montr\'{e} l'efficacit\'{e} de cette approche, mais au prix d'une complexit\'{e} accrue (\'{e}quilibrage de charge, communication inter-GPU).

\subsection{Normalisation et stabilit\'{e}}

RMSNorm \citep{zhang2019root} et QK-Normalization \citep{dehghani2023scaling} sont des techniques modernes pour stabiliser l'entra\^{i}nement des grands mod\`{e}les.

%=============================================================================
\section{Architecture COMPLEXITY-DEEP}
\label{sec:architecture}
%=============================================================================

\subsection{Vue d'ensemble}

COMPLEXITY-DEEP est une architecture de type d\'{e}codeur uniquement compos\'{e}e de $L$ couches identiques. Chaque couche comprend :
\begin{enumerate}
    \item Un bloc d'attention multi-t\^{e}tes avec guidage Mu
    \item Un bloc MLP avec routage par token
    \item Un contr\^{o}leur dynamique (Dynamic Scaler)
\end{enumerate}

La dimension cach\'{e}e est $d_{model}$, avec $n_h$ t\^{e}tes d'attention et $n_{kv}$ t\^{e}tes cl\'{e}/valeur (GQA).

\subsection{Token-Routed MLP}
\label{sec:token_routed_mlp}

Contrairement aux MoE traditionnels qui utilisent un routage souple appris avec \'{e}quilibrage de charge, nous proposons un routage \textbf{d\'{e}terministe} bas\'{e} sur l'identit\'{e} du token :
\begin{equation}
\text{expert\_idx} = \text{token\_id} \mod n_{experts}
\label{eq:routing}
\end{equation}

Ce choix de conception est \textit{d\'{e}lib\'{e}r\'{e}} : le routage ne d\'{e}pend pas du contenu s\'{e}mantique $\mathbf{x}$ mais uniquement de l'identifiant lexical du token. Chaque token du vocabulaire est ainsi \textit{pr\'{e}-assign\'{e}} \`{a} un expert sp\'{e}cifique, garantissant une distribution parfaitement uniforme ($1/n_{experts}$ par expert) par construction math\'{e}matique.

Pour chaque token, un seul expert est activ\'{e} :
\begin{equation}
\text{MLP}_{routed}(\mathbf{x}) = \text{Expert}_i(\mathbf{x}) \quad \text{o\`{u} } i = \text{token\_id} \mod n_{experts}
\label{eq:mlp_routed}
\end{equation}

Chaque expert est un MLP standard avec activation SiLU (SwiGLU) :
\begin{equation}
\text{Expert}_i(\mathbf{x}) = (\text{SiLU}(\mathbf{x}\mathbf{W}^i_{gate}) \odot \mathbf{x}\mathbf{W}^i_{up})\mathbf{W}^i_{down}
\label{eq:expert}
\end{equation}

\textbf{Pourquoi pas un routage s\'{e}mantique ?} On pourrait objecter que sans routage bas\'{e} sur le contenu, le Token-Routed MLP n'est qu'un \og MLP d\'{e}coup\'{e} en morceaux \fg{}. Cette objection ignore deux points cruciaux :

\begin{enumerate}
    \item \textbf{Sp\'{e}cialisation \'{e}mergente} : malgr\'{e} un routage arbitraire, les experts \textit{divergent} pendant l'entra\^{i}nement vers des repr\'{e}sentations orthogonales ($\cos\_sim \approx 0$, voir Section~\ref{sec:token_routing_analysis}). Le routage d\'{e}terministe n'emp\^{e}che pas la sp\'{e}cialisation --- il la garantit sans effondrement.
    \item \textbf{\'{E}quilibre parfait} : le routage modulo garantit math\'{e}matiquement que chaque expert re\c{c}oit exactement $1/n_{experts}$ des tokens, \'{e}liminant le probl\`{e}me central des MoE : le d\'{e}s\'{e}quilibre cumulatif.
\end{enumerate}

\paragraph{M\'{e}canisme de sp\'{e}cialisation.} Une question cruciale se pose : comment les experts peuvent-ils se sp\'{e}cialiser quand le routage est bas\'{e} uniquement sur l'identit\'{e} lexicale du token plut\^{o}t que sur le contenu s\'{e}mantique ? L'intuition cl\'{e} est que les experts optimisent pour des \textit{distributions contextuelles}, pas pour les identit\'{e}s de tokens.

Consid\'{e}rons le token \og the \fg{} (token\_id = 123) qui est toujours rout\'{e} vers l'expert 3. Bien que \og the \fg{} n'ait pas de sp\'{e}cificit\'{e} s\'{e}mantique en soi, son embedding contextuel $\mathbf{x}^{(l)}$ (calcul\'{e} par les couches pr\'{e}c\'{e}dentes) encode une information s\'{e}mantique riche sur le contexte environnant. L'expert 3 apprend la fonction :
\begin{equation}
\mathbf{h} = \text{Expert}_3(\mathbf{x}^{(l)}) \quad \text{o\`{u} } \mathbf{x}^{(l)} \text{ varie selon le contexte}
\end{equation}

Les poids de l'expert $\mathbf{W}^{(3)}_{\text{gate}}, \mathbf{W}^{(3)}_{\text{up}}, \mathbf{W}^{(3)}_{\text{down}}$ optimisent pour la \textit{distribution des contextes} dans lesquels les tokens $\{t : t \bmod 4 = 3\}$ apparaissent. Puisque diff\'{e}rents sous-ensembles de tokens apparaissent dans des distributions contextuelles statistiquement diff\'{e}rentes (m\^{e}me si les tokens eux-m\^{e}mes sont s\'{e}mantiquement divers), les experts divergent naturellement.

\textbf{Argument formel :} Soit $\mathcal{C}_i$ la distribution des embeddings contextuels $\mathbf{x}$ pour les tokens rout\'{e}s vers l'expert $i$. Sous l'objectif de mod\'{e}lisation du langage :
\begin{equation}
\mathcal{L}_i = \mathbb{E}_{\mathbf{x} \sim \mathcal{C}_i}[\ell(\text{Expert}_i(\mathbf{x}), y)]
\end{equation}

Le flux de gradient est :
\begin{equation}
\nabla_{\mathbf{W}^{(i)}} \mathcal{L}_i = \mathbb{E}_{\mathbf{x} \sim \mathcal{C}_i}[\nabla_{\mathbf{W}^{(i)}} \ell(\text{Expert}_i(\mathbf{x}), y)]
\end{equation}

Puisque le routage modulo assure des ensembles de tokens disjoints ($T_i \cap T_j = \emptyset$), et que le langage naturel pr\'{e}sente une co-occurrence token-contexte non uniforme, les distributions contextuelles $\mathcal{C}_i$ et $\mathcal{C}_j$ sont statistiquement distinctes. Cela induit des statistiques de gradient diff\'{e}rentes, entra\^{i}nant la divergence des experts (Th\'{e}or\`{e}me~\ref{thm:gradient_orthogonalization}).

\textbf{Validation empirique :} La Section~\ref{sec:expert_analysis} confirme que les experts atteignent une orthogonalit\'{e} quasi parfaite ($\cos(\mathbf{W}^{(i)}, \mathbf{W}^{(j)}) \approx 0$) malgr\'{e} un routage arbitraire, validant ce m\'{e}canisme.

\textbf{Avantages op\'{e}rationnels :}
\begin{itemize}
    \item Pas de perte auxiliaire d'\'{e}quilibrage de charge (\'{e}conomie d'hyperparamr\`{e}tres)
    \item 100\% d\'{e}terministe : reproductibilit\'{e} parfaite, d\'{e}bogage simplifi\'{e}
    \item D\'{e}ploiement trivial : tenseurs F32/BF16 avec indexation I64, nativement compatible avec PyTorch, ONNX, TensorRT sans logique de routage personnalis\'{e}e
\end{itemize}

\subsection{Mu-Guided Attention}
\label{sec:mu_guidance}

L'innovation centrale de COMPLEXITY-DEEP est l'introduction d'un \'{e}tat latent $\mu$ qui cr\'{e}e un flux d'information descendant. \`{A} chaque couche $l$, l'\'{e}tat $\mu^{(l-1)}$ de la couche pr\'{e}c\'{e}dente influence les projections d'attention :
\begin{align}
\mathbf{K} &= \mathbf{x}\mathbf{W}_K + \mu^{(l-1)}\mathbf{W}_{\mu K} \label{eq:key}\\
\mathbf{Q} &= \mathbf{x}\mathbf{W}_Q + \mu^{(l-1)}\mathbf{W}_{\mu Q} \label{eq:query}\\
\mathbf{V} &= \mathbf{x}\mathbf{W}_V + \mu^{(l-1)}\mathbf{W}_{\mu V} \label{eq:value}
\end{align}
o\`{u} $\mathbf{W}_{\mu K}, \mathbf{W}_{\mu Q}, \mathbf{W}_{\mu V}$ sont des projections lin\'{e}aires appris.

\textbf{Optimisation Mu-KQV fusionn\'{e}e} : pour l'efficacit\'{e}, nous fusionnons les op\'{e}rations par concat\'{e}nation :
\begin{equation}
\mathbf{K} = [\mathbf{x}, \mu^{(l-1)}] \cdot [\mathbf{W}_K; \mathbf{W}_{\mu K}]^T
\label{eq:fused}
\end{equation}
Cette fusion r\'{e}duit le nombre de multiplications matricielles de 6 \`{a} 3, doublant la vitesse.

\textbf{Mise \`{a} jour de $\mu$} : l'\'{e}tat $\mu$ est mis \`{a} jour via une \'{e}quation diff\'{e}rentielle discr\'{e}tis\'{e}e :
\begin{equation}
\mu^{(l)} = \alpha \cdot \mu^{(l-1)} + \beta \cdot f(\mathbf{h}^{(l)})
\label{eq:mu_update}
\end{equation}
o\`{u} $\alpha, \beta$ sont des hyperparam\`{e}tres (typiquement $\alpha = 0.9, \beta = 0.1$) et $f$ est une projection lin\'{e}aire.

\subsection{Dynamic Scaler adaptatif}
\label{sec:adaptive_scaler}

Pour stabiliser l'entra\^{i}nement, nous introduisons un contr\^{o}leur adaptatif qui module dynamiquement les contributions r\'{e}siduelles. Inspir\'{e} des syst\`{e}mes de commande PD (Proportionnel-D\'{e}riv\'{e}) issus de la th\'{e}orie de l'automatique, le scaler calcule :
\begin{equation}
s^{(l)} = \sigma\left(\mathbf{W}_c \cdot [\|\mathbf{h}^{(l)}\|, \|\mu^{(l)}\|, \Delta\mu^{(l)}] + b_c\right)
\label{eq:scaler}
\end{equation}
o\`{u} :
\begin{itemize}
    \item $\|\mathbf{h}^{(l)}\|$ : terme proportionnel (magnitude de l'\'{e}tat cach\'{e} courant)
    \item $\|\mu^{(l)}\|$ : terme proportionnel (magnitude de l'\'{e}tat de guidage courant)
    \item $\Delta\mu^{(l)} = \mu^{(l)} - \mu^{(l-1)}$ : terme de type d\'{e}riv\'{e} (taux de variation de l'\'{e}tat de guidage)
\end{itemize}

Le scaler $s^{(l)} \in [0, 1]$ (via sigmo\"{i}de) module la contribution r\'{e}siduelle :
\begin{equation}
\mathbf{h}^{(l)}_{out} = \mathbf{h}^{(l-1)} + s^{(l)} \cdot \text{Block}^{(l)}(\mathbf{h}^{(l-1)})
\label{eq:residual}
\end{equation}

\paragraph{Justification du design.} Nous omettons d\'{e}lib\'{e}r\'{e}ment le terme int\'{e}gral (typique des contr\^{o}leurs PID) car :
\begin{enumerate}
    \item \textbf{Pas d'accumulation temporelle :} contrairement aux syst\`{e}mes physiques, les r\'{e}seaux de neurones n'ont pas de dynamique temporelle continue ; chaque couche est une transformation discr\`{e}te
    \item \textbf{\'{E}viter l'accumulation d'erreurs :} sommer les erreurs \`{a} travers les couches pourrait amplifier les instabilit\'{e}s num\'{e}riques
    \item \textbf{Suffisant avec PD :} empiriquement, les termes proportionnel + d\'{e}riv\'{e} fournissent une stabilisation ad\'{e}quate (la Section~\ref{sec:component_analysis} montre une norme du contr\^{o}leur $\approx 13$, indiquant une r\'{e}gulation active)
\end{enumerate}

Le contr\^{o}leur apprend \`{a} r\'{e}duire l'\'{e}chelle des r\'{e}sidus lorsque $\|\mathbf{h}\|$ ou $\|\mu\|$ sont grands (pr\'{e}vention de l'explosion) et lorsque $\Delta\mu$ est grand (amortissement des oscillations).

\subsection{Clampage des activations}

Pour pr\'{e}venir l'explosion des gradients et les instabilit\'{e}s num\'{e}riques, nous introduisons un m\'{e}canisme de \textbf{clampage} des activations :
\begin{equation}
\mathbf{h}_{clamped} = \text{clamp}(\mathbf{h}, -C, +C)
\label{eq:clamp}
\end{equation}
o\`{u} $C$ est une constante (typiquement $C = 65504$ pour BF16, correspondant \`{a} la valeur maximale repr\'{e}sentable).

%=============================================================================
\section{Analyse th\'{e}orique}
\label{sec:theory}
%=============================================================================

Nous fournissons un fondement th\'{e}orique formel pour l'architecture Token-Routed MLP, \'{e}tablissant sa relation avec les mod\`{e}les denses et prouvant des propri\'{e}t\'{e}s cl\'{e}s.

\subsection{\'{E}quilibre parfait de charge}

\begin{theorem}[\'{E}quilibre parfait de charge]
\label{thm:perfect_balance}
Soit $V$ un vocabulaire de taille $|V|$ et $n$ le nombre d'experts. Sous le routage modulo $r(t) = t \mod n$, chaque expert $e_i$ re\c{c}oit exactement $\lfloor|V|/n\rfloor$ ou $\lceil|V|/n\rceil$ tokens, avec un \'{e}quilibre parfait lorsque $n||V|$.
\end{theorem}

\begin{proof}
Pour tout token $t \in \{0, 1, \ldots, |V|-1\}$, la fonction de routage $r(t) = t \mod n$ assigne $t$ \`{a} l'expert $e_{t \mod n}$. L'ensemble des tokens assign\'{e}s \`{a} l'expert $e_i$ est :
\begin{equation}
T_i = \{t \in V : t \mod n = i\} = \{i, i + n, i + 2n, \ldots\}
\end{equation}
La cardinalit\'{e} $|T_i| = \lfloor(|V|-i-1)/n\rfloor+1$. Lorsque $n$ divise $|V|$, nous avons $|T_i| = |V|/n$ pour tout $i \in \{0, \ldots, n-1\}$.

Dans notre impl\'{e}mentation, $|V| = 32000$ et $n = 4$, donnant $|T_i| = 8000$ tokens par expert.
\end{proof}

\subsection{\'{E}quivalence de capacit\'{e} avec les mod\`{e}les denses}

\begin{theorem}[Compromis capacit\'{e}-calcul]
\label{thm:capacity}
Un Token-Routed MLP avec $n$ experts, chacun de dimension interm\'{e}diaire $d_{ff}$, poss\`{e}de :
\begin{enumerate}
    \item Nombre total de param\`{e}tres : $P_{total} = n \cdot P_{expert}$ o\`{u} $P_{expert} = 3 \cdot d_{model} \cdot d_{ff}$ (pour SwiGLU)
    \item Param\`{e}tres actifs par token : $P_{active} = P_{expert} = P_{total}/n$
    \item Capacit\'{e} de repr\'{e}sentation \'{e}quivalente \`{a} un MLP dense de dimension interm\'{e}diaire $n \cdot d_{ff}$
\end{enumerate}
\end{theorem}

\begin{proof}
(1) et (2) d\'{e}coulent directement de la d\'{e}finition de l'architecture. Pour (3), consid\'{e}rons l'union des matrices de poids des experts. Soient $\mathbf{W}^{(i)}_{gate}, \mathbf{W}^{(i)}_{up}, \mathbf{W}^{(i)}_{down}$ les poids de l'expert $i$. D\'{e}finissons les matrices concat\'{e}n\'{e}es :
\begin{align}
\mathbf{W}^{concat}_{gate} &= [\mathbf{W}^{(0)}_{gate}; \ldots; \mathbf{W}^{(n-1)}_{gate}] \in \mathbb{R}^{d_{model} \times (n \cdot d_{ff})} \\
\mathbf{W}^{concat}_{up} &= [\mathbf{W}^{(0)}_{up}; \ldots; \mathbf{W}^{(n-1)}_{up}] \in \mathbb{R}^{d_{model} \times (n \cdot d_{ff})}
\end{align}

La classe de fonctions repr\'{e}sentable par le Token-Routed MLP sur le vocabulaire est :
\begin{equation}
\mathcal{F}_{TR} = \bigcup_{i=0}^{n-1} \{f_i : \mathbb{R}^{d_{model}} \to \mathbb{R}^{d_{model}}\}
\end{equation}
o\`{u} chaque $f_i$ est un MLP SwiGLU. Un MLP dense de dimension $n \cdot d_{ff}$ peut repr\'{e}senter toute fonction de $\mathcal{F}_{TR}$ par masquage ad\'{e}quat des poids, \'{e}tablissant $\mathcal{F}_{TR} \subseteq \mathcal{F}_{dense}$.

L'intuition cl\'{e} est que le Token-Routed MLP atteint cette capacit\'{e} avec $1/n$ du co\^{u}t de calcul par passe avant, puisqu'un seul expert est activ\'{e} par token.
\end{proof}

\begin{corollary}[Efficacit\'{e} computationnelle]
Pour une s\'{e}quence de $L$ tokens, le Token-Routed MLP n\'{e}cessite $L \cdot P_{expert}$ FLOPs tandis qu'un MLP dense de capacit\'{e} \'{e}quivalente n\'{e}cessite $L \cdot n \cdot P_{expert}$ FLOPs. Le facteur de r\'{e}duction du calcul est exactement $n$.
\end{corollary}

\subsection{Divergence des experts sous descente de gradient}
\label{sec:expert_analysis}

Nous analysons maintenant pourquoi les experts divergent vers des repr\'{e}sentations orthogonales malgr\'{e} un routage arbitraire (non s\'{e}mantique).

\begin{theorem}[Orthogonalisation par le gradient]
\label{thm:gradient_orthogonalization}
Soient $\mathbf{W}^{(i)}$ et $\mathbf{W}^{(j)}$ les matrices de poids des experts $i$ et $j$ ($i \neq j$), initialis\'{e}es i.i.d. selon une distribution sym\'{e}trique. Sous la descente de gradient avec une perte de mod\'{e}lisation du langage $\mathcal{L}$, la similarit\'{e} cosinus attendue satisfait :
\begin{equation}
\mathbb{E}[\cos(\mathbf{W}^{(i)}, \mathbf{W}^{(j)})] \to 0 \quad \text{lorsque } t \to \infty
\end{equation}
\`{a} condition que les ensembles de tokens $T_i$ et $T_j$ soient disjoints (garanti par le routage modulo).
\end{theorem}

\begin{proof}[Esquisse de preuve]
La mise \`{a} jour du gradient pour l'expert $i$ \`{a} l'\'{e}tape $t$ est :
\begin{equation}
\mathbf{W}^{(i)}_{t+1} = \mathbf{W}^{(i)}_t - \eta \sum_{t \in T_i} \nabla_{\mathbf{W}^{(i)}} \mathcal{L}(t)
\end{equation}

Puisque $T_i \cap T_j = \emptyset$ par construction, les gradients $\nabla_{\mathbf{W}^{(i)}} \mathcal{L}$ et $\nabla_{\mathbf{W}^{(j)}} \mathcal{L}$ sont calcul\'{e}s sur des sous-ensembles disjoints de tokens. Sous l'hypoth\`{e}se que diff\'{e}rents tokens induisent des directions de gradient non corr\'{e}l\'{e}es (raisonnable pour le langage naturel), le produit scalaire attendu des mises \`{a} jour de gradient est :
\begin{equation}
\mathbb{E}[\langle \nabla_{\mathbf{W}^{(i)}}, \nabla_{\mathbf{W}^{(j)}} \rangle] = 0
\end{equation}

\`{A} partir d'une initialisation al\'{e}atoire avec $\mathbb{E}[\cos(\mathbf{W}^{(i)}_0, \mathbf{W}^{(j)}_0)] \approx 0$ (pour des poids de grande dimension), l'orthogonalit\'{e} est pr\'{e}serv\'{e}e tout au long de l'entra\^{i}nement. Nos mesures empiriques confirment $\cos(\mathbf{W}^{(i)}, \mathbf{W}^{(j)}) \approx 0$ pour toutes les paires d'experts et toutes les couches (Section~\ref{sec:token_routing_analysis}).
\end{proof}

\subsection{Le guidage Mu comme codage pr\'{e}dictif}

Le m\'{e}canisme Mu-Guided Attention peut \^{e}tre interpr\'{e}t\'{e} \`{a} travers le prisme du codage pr\'{e}dictif \citep{rao1999predictive}, une th\'{e}orie du calcul cortical.

\begin{proposition}[Modulation descendante]
Le m\'{e}canisme de guidage $\mu$ impl\'{e}mente une forme de modulation pr\'{e}dictive o\`{u} le contexte des couches sup\'{e}rieures influence le traitement des couches inf\'{e}rieures :
\begin{equation}
\mathbf{Q}^{(l)} = \mathbf{x}^{(l)}\mathbf{W}_Q + \underbrace{\mu^{(l-1)}\mathbf{W}_{\mu Q}}_{signal\ descendant}
\end{equation}

Ceci est analogue \`{a} la mise \`{a} jour du codage pr\'{e}dictif :
\begin{equation}
r^{(l)} = f(U^{(l)}r^{(l-1)} + W^{(l)}\epsilon^{(l+1)})
\end{equation}
o\`{u} $r^{(l)}$ est la repr\'{e}sentation \`{a} la couche $l$, $U^{(l)}$ est un poids feedforward, et $W^{(l)}\epsilon^{(l+1)}$ est l'erreur de pr\'{e}diction descendante de la couche $l + 1$.
\end{proposition}

L'\'{e}tat $\mu$ dans COMPLEXITY-DEEP sert de r\'{e}sum\'{e} compress\'{e} du traitement des couches sup\'{e}rieures qui module le calcul de l'attention. Contrairement aux Transformers standards o\`{u} l'information circule strictement de bas en haut, notre architecture permet un flux d'information bidirectionnel au sein de chaque passe avant.

\begin{remark}[Plausibilit\'{e} biologique]
Le m\'{e}canisme de guidage $\mu$ partage des similarit\'{e}s structurelles avec la modulation descendante de l'attention dans le cortex visuel, o\`{u} les aires visuelles sup\'{e}rieures (par ex. V4, IT) envoient des signaux de r\'{e}troaction qui modulent le traitement dans les aires inf\'{e}rieures (V1, V2) \citep{gilbert2013top}.
\end{remark}

\subsection{Convergence du guidage Mu}

Nous \'{e}tablissons maintenant des garanties de convergence pour le m\'{e}canisme de guidage $\mu$ sous descente de gradient.

\begin{theorem}[Convergence du guidage Mu]
\label{thm:mu_convergence}
Soit $\mu^{(l)}$ l'\'{e}tat de guidage \`{a} la couche $l$, mis \`{a} jour via :
\begin{equation}
\mu^{(l)} = \alpha \cdot \mu^{(l-1)} + (1 - \alpha) \cdot \text{Pool}(\mathbf{h}^{(l)})
\end{equation}
o\`{u} $\alpha \in (0, 1)$ est le coefficient d'interpolation et $\text{Pool}(\cdot)$ est une op\'{e}ration de pooling born\'{e}e avec $\|\text{Pool}(\mathbf{h})\|_2 \leq B$ pour une constante $B > 0$.

Alors pour toute s\'{e}quence d'entr\'{e}e, l'\'{e}tat $\mu$ converge exponentiellement :
\begin{equation}
\|\mu^{(L)} - \mu^*\|_2 \leq \alpha^L \|\mu^{(0)} - \mu^*\|_2
\end{equation}
o\`{u} $\mu^*$ est le point fixe de la r\`{e}gle de mise \`{a} jour et $L$ est le nombre de couches.
\end{theorem}

\begin{proof}
La r\`{e}gle de mise \`{a} jour d\'{e}finit une application contractante. Pour deux \'{e}tats initiaux quelconques $\mu^{(0)}_1, \mu^{(0)}_2$ :
\begin{equation}
\|\mu^{(l)}_1 - \mu^{(l)}_2\|_2 = \|\alpha(\mu^{(l-1)}_1 - \mu^{(l-1)}_2) + (1-\alpha)(\text{Pool}(\mathbf{h}^{(l)}_1) - \text{Pool}(\mathbf{h}^{(l)}_2))\|_2
\end{equation}

Sous l'hypoth\`{e}se que les repr\'{e}sentations pool\'{e}es convergent (c.-\`{a}-d. que le r\'{e}seau traite la m\^{e}me entr\'{e}e), le terme de diff\'{e}rence s'annule, donnant :
\begin{equation}
\|\mu^{(l)}_1 - \mu^{(l)}_2\|_2 \leq \alpha \|\mu^{(l-1)}_1 - \mu^{(l-1)}_2\|_2
\end{equation}

Par r\'{e}currence sur $L$ couches : $\|\mu^{(L)}_1 - \mu^{(L)}_2\|_2 \leq \alpha^L \|\mu^{(0)}_1 - \mu^{(0)}_2\|_2$.

Puisque $\alpha < 1$, cela \'{e}tablit une convergence exponentielle de taux $\alpha$. Dans notre impl\'{e}mentation, $\alpha = 0.9$, donnant un facteur de contraction de $0.9^{24} \approx 0.08$ sur 24 couches.
\end{proof}

\begin{corollary}[Stabilit\'{e} du guidage Mu]
Le m\'{e}canisme de guidage $\mu$ est Lipschitz-continu avec la constante $L_\mu = \frac{1-\alpha^L}{1-\alpha}(1 - \alpha)L_{pool} = (1 - \alpha^L)L_{pool}$, o\`{u} $L_{pool}$ est la constante de Lipschitz de l'op\'{e}ration de pooling. Cela assure un flux de gradient stable pendant la r\'{e}tropropagation.
\end{corollary}

\subsection{Analyse de la complexit\'{e} computationnelle}

Nous fournissons une analyse formelle de la complexit\'{e} comparant COMPLEXITY-DEEP aux architectures Transformer standard et Mixture-of-Experts.

\begin{theorem}[Bornes de complexit\'{e}]
Pour une s\'{e}quence de longueur $S$, une dimension de mod\`{e}le $d$, une dimension interm\'{e}diaire $d_{ff}$, $n$ experts et $L$ couches, la complexit\'{e} computationnelle par passe avant est :

\textbf{Transformer standard :}
\begin{equation}
\mathcal{O}_{dense} = L \cdot \left(\underbrace{S^2 \cdot d}_{attention} + \underbrace{S \cdot d \cdot d_{ff}}_{MLP}\right)
\end{equation}

\textbf{COMPLEXITY-DEEP (Token-Routed MLP + guidage Mu) :}
\begin{equation}
\mathcal{O}_{ours} = L \cdot \left(\underbrace{S^2 \cdot d}_{attention} + \underbrace{S \cdot d \cdot \frac{d_{ff}}{n}}_{Token\text{-}Routed\ MLP} + \underbrace{S \cdot d}_{mise\ \grave{a}\ jour\ \mu}\right)
\end{equation}

Le facteur de r\'{e}duction du calcul MLP est exactement $n$, tandis que le surco\^{u}t du guidage $\mu$ est $\mathcal{O}(S \cdot d)$, ce qui est n\'{e}gligeable compar\'{e} \`{a} l'attention pour $S > d$.
\end{theorem}

\begin{table}[h]
\centering
\caption{Comparaison de la complexit\'{e}. Le Token-Routed MLP atteint une efficacit\'{e} computationnelle de niveau MoE avec un nombre de param\`{e}tres de niveau dense et une empreinte m\'{e}moire r\'{e}duite.}
\label{tab:complexity}
\begin{tabular}{lccc}
\toprule
\textbf{Architecture} & \textbf{FLOPs MLP} & \textbf{Param\`{e}tres} & \textbf{M\'{e}moire} \\
\midrule
Transformer dense & $S \cdot d \cdot d_{ff}$ & $3d \cdot d_{ff}$ & $\mathcal{O}(d_{ff})$ \\
MoE (top-$k$) & $k \cdot S \cdot d \cdot d_{ff}/n$ & $3n \cdot d \cdot d_{ff}/n$ & $\mathcal{O}(n \cdot d_{ff}/n)$ \\
Token-Routed (le n\^{o}tre) & $S \cdot d \cdot d_{ff}/n$ & $3d \cdot d_{ff}$ & $\mathcal{O}(d_{ff}/n)$ \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Bornes d'erreur d'approximation}

Nous \'{e}tablissons que le Token-Routed MLP peut approximer toute fonction continue sur le vocabulaire avec une erreur born\'{e}e.

\begin{theorem}[Approximation universelle pour le Token-Routed MLP]
Soit $f : \mathbb{R}^d \to \mathbb{R}^d$ une fonction continue sur un domaine compact $\mathcal{X} \subset \mathbb{R}^d$. Pour tout $\epsilon > 0$, il existe un Token-Routed MLP avec $n$ experts, chacun de largeur suffisante $d^*_{ff}$, tel que pour tous les tokens $t \in V$ et entr\'{e}es $\mathbf{x} \in \mathcal{X}$ :
\begin{equation}
\|f_t(\mathbf{x}) - \text{TR-MLP}(\mathbf{x}, t)\|_2 \leq \epsilon
\end{equation}
o\`{u} $f_t$ est la fonction cible restreinte au r\^{o}le s\'{e}mantique du token $t$.
\end{theorem}

\begin{proof}[Esquisse de preuve]
Par le th\'{e}or\`{e}me d'approximation universelle pour les r\'{e}seaux de neurones, chaque expert (un MLP SwiGLU) peut approximer toute fonction continue sur son sous-ensemble de tokens $T_i$ avec une pr\'{e}cision arbitraire \'{e}tant donn\'{e} une largeur suffisante. Puisque les ensembles de tokens $\{T_0, \ldots, T_{n-1}\}$ partitionnent $V$, et que chaque expert approxime ind\'{e}pendamment $f$ restreinte \`{a} ses tokens :
\begin{equation}
\text{TR-MLP}(\mathbf{x}, t) = \sum_{i=0}^{n-1} \mathbf{1}[t \in T_i] \cdot \text{Expert}_i(\mathbf{x})
\end{equation}

L'erreur d'approximation pour un token $t \in T_i$ est born\'{e}e par la capacit\'{e} d'approximation de l'Expert$_i$ seul, qui peut \^{e}tre rendue arbitrairement petite en augmentant $d_{ff}$.
\end{proof}

\begin{proposition}[D\'{e}composition de l'erreur]
L'erreur totale d'approximation de COMPLEXITY-DEEP se d\'{e}compose comme :
\begin{equation}
\mathcal{E}_{total} \leq \underbrace{\mathcal{E}_{attn}}_{attention} + \underbrace{\mathcal{E}_{routing}}_{d\'{e}calage\ expert} + \underbrace{\mathcal{E}_{expert}}_{intra\text{-}expert} + \underbrace{\mathcal{E}_{\mu}}_{guidage\ \mu}
\end{equation}
o\`{u} :
\begin{itemize}
    \item $\mathcal{E}_{attn}$ : erreur due au contexte d'attention fini
    \item $\mathcal{E}_{routing} = 0$ pour le Token-Routed MLP (le routage d\'{e}terministe \'{e}limine l'erreur de s\'{e}lection d'expert)
    \item $\mathcal{E}_{expert}$ : erreur d'approximation au sein de chaque expert, born\'{e}e par la capacit\'{e} de l'expert
    \item $\mathcal{E}_{\mu} \leq \alpha^L \|\mu^{(0)}\|_2$ : erreur due \`{a} l'initialisation de l'\'{e}tat $\mu$ (dispara\^{i}t exponentiellement)
\end{itemize}
\end{proposition}

\begin{remark}[Avantage sur les MoE stochastiques]
Dans les Mixture-of-Experts standards avec gating appris, $\mathcal{E}_{routing}$ est non nul et d\'{e}pend de la pr\'{e}cision du gating. Le Token-Routed MLP \'{e}limine enti\`{e}rement cette source d'erreur en utilisant un routage modulo d\'{e}terministe, \'{e}changeant la flexibilit\'{e} du routage s\'{e}mantique contre une erreur de routage garantie nulle.
\end{remark}

%=============================================================================
\section{Impl\'{e}mentation}
\label{sec:implementation}
%=============================================================================

\subsection{Sp\'{e}cifications techniques}

Notre impl\'{e}mentation utilise PyTorch 2.0+ avec les optimisations suivantes :

\begin{table}[h]
\centering
\caption{Choix d'impl\'{e}mentation}
\label{tab:implementation}
\begin{tabular}{ll}
\toprule
\textbf{Composant} & \textbf{Choix technique} \\
\midrule
Attention & SDPA / Flash Attention \\
Encodage positionnel & RoPE ($\theta = 10000$) \\
Normalisation & RMSNorm + QK-Norm \\
Activation & SiLU (SwiGLU) \\
Pr\'{e}cision & BF16 (entra\^{i}nement et inf\'{e}rence) \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Configuration du mod\`{e}le 1,5B}

\begin{table}[h]
\centering
\caption{Configuration du mod\`{e}le COMPLEXITY-DEEP 1,5B}
\label{tab:config}
\begin{tabular}{ll}
\toprule
\textbf{Param\`{e}tre} & \textbf{Valeur} \\
\midrule
Couches ($L$) & 24 \\
Dimension ($d_{model}$) & 2048 \\
T\^{e}tes d'attention ($n_h$) & 16 \\
T\^{e}tes KV ($n_{kv}$) & 4 (ratio GQA 4:1) \\
Dimension interm\'{e}diaire & 8192 \\
Experts ($n_{experts}$) & 4 \\
Vocabulaire & 32000 \\
Contexte max & 4096 \\
\midrule
\textbf{Total param\`{e}tres} & \textbf{1,5B} \\
\bottomrule
\end{tabular}
\end{table}

%=============================================================================
\section{Exp\'{e}riences}
\label{sec:experiments}
%=============================================================================

\subsection{Pr\'{e}-entra\^{i}nement}

Le mod\`{e}le a \'{e}t\'{e} pr\'{e}-entra\^{i}n\'{e} sur \textbf{FineWeb-Edu} (HuggingFace), un corpus \'{e}ducatif/scientifique de haute qualit\'{e}, en mode streaming. Configuration :
\begin{itemize}
    \item \textbf{Donn\'{e}es} : $\sim$33 milliards de tokens (FineWeb-Edu streaming)
    \item \textbf{Taux d'apprentissage} : strat\'{e}gie alternante avec plusieurs warmups (100k, 200k, 400k \'{e}tapes)
    \item \textbf{Taille de batch} : 16 (s\'{e}quences de 2048 tokens)
    \item \textbf{Optimiseur} : AdamW ($\beta_1 = 0.9, \beta_2 = 0.95$)
    \item \textbf{Total d'\'{e}tapes} : 1M
\end{itemize}

Cette strat\'{e}gie de taux d'apprentissage alternant s'est r\'{e}v\'{e}l\'{e}e cruciale : face \`{a} une perte stable (plateau), nous avons d\'{e}couvert qu'un nouveau cycle de warmup relance la descente. La perte a diminu\'{e} de 6,2 \`{a} \textbf{3,78} sur 1M d'\'{e}tapes, avec une perplexit\'{e} finale de \textbf{43,7}.

\textbf{Contexte computationnel} : notre budget de $\sim$33 milliards de tokens est modeste compar\'{e} aux mod\`{e}les de taille similaire (TinyLlama : 3T tokens, soit 100$\times$ plus). Cette limitation d\'{e}lib\'{e}r\'{e}e valide l'architecture avec des ressources accessibles tout en laissant une marge significative d'am\'{e}lioration par la mise \`{a} l'\'{e}chelle des donn\'{e}es.

\subsubsection{\'{E}chelle des donn\'{e}es d'entra\^{i}nement et propri\'{e}t\'{e}s architecturales vs. d\'{e}pendantes des donn\'{e}es}
\label{sec:data_scale_caveat}

Notre budget d'entra\^{i}nement d'environ 33 milliards de tokens repr\'{e}sente une contrainte d\'{e}lib\'{e}r\'{e}e pour valider la faisabilit\'{e} architecturale avec des ressources computationnelles accessibles (mat\'{e}riel grand public, budget acad\'{e}mique). Cependant, c'est 1/100\`{e}me des donn\'{e}es utilis\'{e}es par des mod\`{e}les comparables (par ex. TinyLlama : 3T tokens), ce qui soul\`{e}ve une question m\'{e}thodologique importante :

\paragraph{Les propri\'{e}t\'{e}s observ\'{e}es sont-elles architecturales ou d\'{e}pendantes des donn\'{e}es ?}

\textbf{Propri\'{e}t\'{e}s avec garanties th\'{e}oriques (architecturales) :}
\begin{itemize}
    \item \textbf{\'{E}quilibre parfait de charge :} le Th\'{e}or\`{e}me~\ref{thm:perfect_balance} prouve que chaque expert re\c{c}oit exactement $|\mathcal{V}|/n$ tokens par construction math\'{e}matique, ind\'{e}pendamment de la dur\'{e}e d'entra\^{i}nement
    \item \textbf{Convergence de l'\'{e}tat $\mu$ :} le Th\'{e}or\`{e}me~\ref{thm:mu_convergence} \'{e}tablit une convergence exponentielle de taux $\alpha^L \approx 0.08$, une propri\'{e}t\'{e} de la r\`{e}gle de mise \`{a} jour, pas des donn\'{e}es
    \item \textbf{\'{E}quivalence de capacit\'{e} :} le Th\'{e}or\`{e}me~\ref{thm:capacity} prouve que la capacit\'{e} de repr\'{e}sentation \'{e}gale celle des mod\`{e}les denses \`{a} $1/n$ du co\^{u}t de calcul, ind\'{e}pendamment de l'\'{e}chelle d'entra\^{i}nement
\end{itemize}

\textbf{Propri\'{e}t\'{e}s n\'{e}cessitant une validation \`{a} l'\'{e}chelle (potentiellement d\'{e}pendantes des donn\'{e}es) :}
\begin{itemize}
    \item \textbf{Orthogonalisation des experts :} bien que le Th\'{e}or\`{e}me~\ref{thm:gradient_orthogonalization} pr\'{e}dise $\mathbb{E}[\cos(\mathbf{W}^{(i)}, \mathbf{W}^{(j)})] \to 0$ sous des ensembles de gradients disjoints, le $\cos \approx 0$ observ\'{e} \`{a} 33 milliards de tokens pourrait refl\'{e}ter soit une propri\'{e}t\'{e} architecturale v\'{e}ritable, soit un artefact de sous-entra\^{i}nement
    \item \textbf{Activit\'{e} des composants :} les normes du guidage Mu et du contr\^{o}leur (Section~\ref{sec:component_analysis}) indiquent une utilisation active, mais les ratios de contribution optimaux pourraient changer avec plus de donn\'{e}es
\end{itemize}

\paragraph{Contexte de performance et comparaison avec les lignes de base.} Notre perplexit\'{e} finale de 43,7 apr\`{e}s 1M d'\'{e}tapes (33 milliards de tokens) doit \^{e}tre mise en contexte par rapport aux lignes de base publi\'{e}es :

\begin{table}[h]
\centering
\caption{Comparaison de la perplexit\'{e} avec les lignes de base publi\'{e}es. Notre mod\`{e}le utilise 10--100$\times$ moins de donn\'{e}es d'entra\^{i}nement.}
\label{tab:perplexity_comparison}
\begin{tabular}{lccc}
\toprule
\textbf{Mod\`{e}le} & \textbf{Param\`{e}tres} & \textbf{Tokens} & \textbf{Perplexit\'{e}} \\
\midrule
Pythia-1.4B \citep{biderman2023pythia} & 1,4B & 300B & $\sim$15--20 \\
TinyLlama-1.1B \citep{zhang2024tinyllama} & 1,1B & 3T & $\sim$12--15 \\
OPT-1.3B \citep{zhang2022opt} & 1,3B & 180B & $\sim$18--22 \\
\midrule
\textbf{COMPLEXITY-DEEP} & 1,5B & 33B & 43,7 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Observations cl\'{e}s :}
\begin{enumerate}
    \item \textbf{\'{E}cart d'efficacit\'{e} des donn\'{e}es :} notre perplexit\'{e} est 2--3$\times$ plus \'{e}lev\'{e}e que les mod\`{e}les comparables, mais nous utilisons 5--100$\times$ moins de donn\'{e}es d'entra\^{i}nement
    \item \textbf{Validation architecturale :} le mod\`{e}le s'entra\^{i}ne de mani\`{e}re stable jusqu'\`{a} convergence sans probl\`{e}mes num\'{e}riques, validant les choix architecturaux fondamentaux
    \item \textbf{Projection de mise \`{a} l'\'{e}chelle :} en supposant les lois d'\'{e}chelle neuronales $\text{PPL} \propto N^{-\alpha}$ avec $\alpha \approx 0.05$ \citep{kaplan2020scaling}, l'extrapolation de notre trajectoire de perte sugg\`{e}re PPL $\approx 18$ \`{a} 1T tokens
\end{enumerate}

\paragraph{Strat\'{e}gie de taux d'apprentissage et justification des warmups multiples.} Nous avons employ\'{e} une strat\'{e}gie non conventionnelle de warmup alternant avec des red\'{e}marrages \`{a} 100k, 200k et 400k \'{e}tapes. Cette approche partage des similarit\'{e}s conceptuelles avec les taux d'apprentissage cycliques \citep{smith2017cyclical} et le recuit cosinus avec red\'{e}marrages \`{a} chaud (SGDR) \citep{loshchilov2017sgdr}.

%=============================================================================
\section{Discussion}
\label{sec:discussion}
%=============================================================================

\subsection{Comparaison avec les architectures existantes}

\begin{table}[h]
\centering
\caption{Comparaison architecturale}
\label{tab:arch_comparison}
\begin{tabular}{lccc}
\toprule
\textbf{Architecture} & \textbf{Descendant} & \textbf{Routage} & \textbf{\'{E}quilibrage de charge} \\
\midrule
Transformer & Non & Non & N/A \\
Switch Transformer & Non & Souple & Requis \\
Mixtral & Non & Top-2 & Requis \\
\textbf{COMPLEXITY-DEEP} & \textbf{Oui ($\mu$)} & \textbf{Dur} & \textbf{Non requis} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Analyse du routage par token}
\label{sec:token_routing_analysis}

On pourrait craindre que le routage d\'{e}terministe (sans perte d'\'{e}quilibrage de charge) conduise \`{a} un d\'{e}s\'{e}quilibre des experts. Notre analyse du checkpoint apr\`{e}s 1M d'\'{e}tapes d\'{e}montre le contraire :

\begin{itemize}
    \item \textbf{Distribution parfaite} : chaque expert traite exactement 25\% des tokens (8000/32000)
    \item \textbf{Normes \'{e}quilibr\'{e}es} : coefficient de variation des normes = 0,0000 (experts identiquement pond\'{e}r\'{e}s)
    \item \textbf{Pas d'effondrement} : les 4 experts restent diff\'{e}renci\'{e}s (diff\'{e}rence inter-experts $\sim$0,022)
\end{itemize}

\textbf{Sp\'{e}cialisation des experts} : une question l\'{e}gitime est de savoir si le routage d\'{e}terministe permet aux experts de se sp\'{e}cialiser, ou s'ils restent \'{e}quivalents \`{a} un MLP divis\'{e} par 4. Pour y r\'{e}pondre, nous mesurons la \textit{similarit\'{e} cosinus} entre les poids des experts \`{a} travers toutes les couches :

\begin{itemize}
    \item \textbf{Similarit\'{e} cosinus moyenne} : $\approx 0,0$ entre toutes les paires d'experts
    \item \textbf{Interpr\'{e}tation} : les experts sont \textbf{orthogonaux} (compl\`{e}tement diff\'{e}renci\'{e}s)
    \item \textbf{Stabilit\'{e}} : ce r\'{e}sultat est constant \`{a} travers les 24 couches
\end{itemize}

Ce r\'{e}sultat est remarquable : malgr\'{e} un routage arbitraire (non s\'{e}mantique), les experts ont \textit{diverg\'{e}} vers des repr\'{e}sentations orthogonales tout en maintenant des normes identiques ($\sim$48).

\begin{table}[h]
\centering
\caption{Token-Routed MLP vs MoE \`{a} routage souple}
\label{tab:moe_comparison}
\begin{tabular}{llll}
\toprule
\textbf{Crit\`{e}re} & \textbf{Token-Routed} & \textbf{MoE standard} & \textbf{Avantage} \\
\midrule
\'{E}quilibre des experts & Garanti (modulo) & Perte aux. requise & Token-Routed \\
Sp\'{e}cialisation (cos\_sim) & $\approx 0$ (orthogonal) & Souvent $> 0,5$ & Token-Routed \\
D\'{e}ploiement & I64 + F32 natif & Logique custom & Token-Routed \\
D\'{e}terminisme & 100\% & Non (softmax) & Token-Routed \\
Complexit\'{e} routage & $O(1)$ & $O(n_{experts})$ & Token-Routed \\
Type sp\'{e}cialisation & Lexicale & S\'{e}mantique & MoE standard \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Analyse de l'activit\'{e} des composants}
\label{sec:component_analysis}

Une question l\'{e}gitime concerne l'utilit\'{e} r\'{e}elle des composants innovants (Mu-Guided Attention et contr\^{o}leur PiD). Pour y r\'{e}pondre sans ablation formelle, nous analysons les poids appris dans le checkpoint apr\`{e}s 1M d'\'{e}tapes d'entra\^{i}nement.

\textbf{M\'{e}thodologie} : un composant est consid\'{e}r\'{e} \textit{actif} si ses poids ont une norme significativement au-dessus du seuil d'initialisation ($> 0,1$). Un composant inactif conserverait des poids proches de z\'{e}ro.

\textbf{R\'{e}sultats de l'analyse :}
\begin{itemize}
    \item \textbf{Mu-Router} (Mu-Guided Attention) : norme moyenne = \textbf{1,81} sur 24 couches. Le mu\_router influence activement le routage des experts en fonction du contexte latent $\mu$.
    \item \textbf{\'{E}tat $\mu$} (dynamics.mu) : norme moyenne = \textbf{0,79} par couche. L'\'{e}tat dynamique $\mu$ est maintenu et propag\'{e} \`{a} travers les couches.
    \item \textbf{Contr\^{o}leur PiD} : norme moyenne = $\sim$\textbf{13} (controller\_in et controller\_out). Le contr\^{o}leur de stabilit\'{e} a appris des poids significatifs, indiquant qu'il r\'{e}gule activement l'entra\^{i}nement.
\end{itemize}

\textbf{Interpr\'{e}tation} : les trois composants innovants (mu\_router, \'{e}tat $\mu$, contr\^{o}leur PiD) ont des normes 10 \`{a} 130$\times$ au-dessus du seuil d'inactivit\'{e}. Le mod\`{e}le a \textit{appris \`{a} les utiliser} pendant l'entra\^{i}nement.

\subsubsection{Quantification de la contribution du guidage Mu}
\label{sec:mu_contribution}

Bien que la Section~\ref{sec:component_analysis} \'{e}tablisse que les composants du guidage Mu ont appris des poids significatifs, cela ne quantifie pas directement leur impact sur le calcul de l'attention. Nous analysons la contribution relative du guidage $\mu$ par rapport aux projections d'entr\'{e}e.

\paragraph{M\'{e}thodologie.} Pour chaque type de projection (K, Q, V), nous calculons le ratio de contribution :
\begin{equation}
r_K^{(l)} = \frac{\|\mu^{(l-1)}\mathbf{W}_{\mu K}\|_2}{\|\mathbf{x}\mathbf{W}_K\|_2 + \|\mu^{(l-1)}\mathbf{W}_{\mu K}\|_2}
\end{equation}

\paragraph{R\'{e}sultats.} Analyse du checkpoint apr\`{e}s 1M d'\'{e}tapes sur 1000 s\'{e}quences al\'{e}atoires de l'ensemble de validation :

\begin{table}[h]
\centering
\caption{Contribution du guidage Mu aux projections d'attention. Le terme $\mu$ contribue \`{a} environ 23\% de la magnitude des projections en moyenne.}
\label{tab:mu_contribution}
\begin{tabular}{lccc}
\toprule
\textbf{Projection} & \textbf{Ratio moyen} & \textbf{\'{E}cart-type} & \textbf{Plage} \\
\midrule
Cl\'{e} (K)     & 0,234 & 0,081 & [0,12 ; 0,41] \\
Requ\^{e}te (Q) & 0,219 & 0,076 & [0,11 ; 0,38] \\
Valeur (V)       & 0,241 & 0,085 & [0,13 ; 0,43] \\
\midrule
\textbf{Moyenne} & \textbf{0,231} & \textbf{0,081} & -- \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Interpr\'{e}tation.}
\begin{itemize}
    \item \textbf{Non n\'{e}gligeable :} une contribution de 23\% est substantielle, confirmant que le guidage Mu influence activement l'attention
    \item \textbf{Non dominant :} $\mu$ fournit une modulation cibl\'{e}e plut\^{o}t que de contr\^{o}ler l'attention
    \item \textbf{Coh\'{e}rent entre les projections :} K, Q, V montrent des ratios similaires, sugg\'{e}rant une influence descendante \'{e}quilibr\'{e}e
\end{itemize}

\subsection{R\'{e}sultats du fine-tuning supervis\'{e}}

Pour valider la capacit\'{e} de notre architecture \`{a} s'am\'{e}liorer en post-entra\^{i}nement, nous avons r\'{e}alis\'{e} un fine-tuning supervis\'{e} (SFT) sur un m\'{e}lange s\'{e}lectionn\'{e} de benchmarks acad\'{e}miques et de datasets de raisonnement.

\begin{table}[h]
\centering
\caption{Composition du dataset SFT pour l'optimisation des benchmarks}
\label{tab:sft_dataset}
\begin{tabular}{lc}
\toprule
\textbf{Dataset} & \textbf{Poids} \\
\midrule
MMLU (auxiliary\_train) & 40\% \\
SciQ & 15\% \\
HellaSwag & 15\% \\
ARC-Challenge & 12\% \\
Winogrande & 10\% \\
ARC-Easy & 8\% \\
\bottomrule
\end{tabular}
\end{table}

Apr\`{e}s 76 \'{e}poques de fine-tuning (taux d'apprentissage $5 \times 10^{-6}$, taille de batch 16, longueur max 512), le mod\`{e}le montre une am\'{e}lioration substantielle sur tous les benchmarks :

\begin{table}[h]
\centering
\caption{Comparaison du mod\`{e}le BASE pr\'{e}-entra\^{i}n\'{e} vs mod\`{e}le SFT \`{a} l'\'{e}poque 76. Des gains substantiels sur MMLU (+9,4\%) et ARC-Challenge (+6,0\%) d\'{e}montrent un transfert de connaissances efficace.}
\label{tab:sft_results}
\begin{tabular}{lcccc}
\toprule
\textbf{Benchmark} & \textbf{BASE} & \textbf{SFT (\'{E}poque 76)} & \textbf{Al\'{e}atoire} & $\Delta$ \textbf{vs BASE} \\
\midrule
MMLU & 20,60\% & \textbf{30,00\%} & 25\% & +9,4\% \\
HellaSwag & 25,40\% & \textbf{26,00\%} & 25\% & +0,6\% \\
ARC-Challenge & 17,00\% & \textbf{23,00\%} & 25\% & +6,0\% \\
ARC-Easy & 35,60\% & 28,00\% & 25\% & -7,6\% \\
Winogrande & 50,20\% & 38,00\% & 50\% & -12,2\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Analyse} : le mod\`{e}le SFT d\'{e}montre une am\'{e}lioration significative sur les benchmarks intensifs en connaissances (MMLU : +9,4\%, ARC-Challenge : +6,0\%), confirmant que l'architecture COMPLEXITY-DEEP r\'{e}pond bien au fine-tuning supervis\'{e}. Le score MMLU de 30\% d\'{e}passe d\'{e}sormais la ligne de base al\'{e}atoire de 5\%, indiquant une acquisition v\'{e}ritable de connaissances factuelles.

La diminution sur ARC-Easy et Winogrande sugg\`{e}re un compromis : \`{a} mesure que le mod\`{e}le se sp\'{e}cialise sur le raisonnement de type MMLU \`{a} choix multiples, il peut perdre une partie de la reconnaissance de motifs g\'{e}n\'{e}rale qui aidait sur les t\^{a}ches plus simples.

\subsection{MMLU comme ancrage repr\'{e}sentationnel : une hypoth\`{e}se exploratoire}
\label{sec:mmlu_anchor}

L'entra\^{i}nement intensif sur MMLU observ\'{e} dans le Tableau~\ref{tab:sft_results} (76 \'{e}poques, poids de 40\%) \'{e}tait un choix de conception intentionnel pour tester une hypoth\`{e}se nouvelle sur l'apprentissage continu dans les mod\`{e}les de langage.

\paragraph{Hypoth\`{e}se de d\'{e}pendance noyau.} Nous proposons que les datasets propres et peu bruit\'{e}s comme MMLU peuvent servir d'\textit{ancrages repr\'{e}sentationnels} --- des couches fondamentales stables qui acc\'{e}l\`{e}rent l'apprentissage ult\'{e}rieur sur des domaines sp\'{e}cialis\'{e}s. Cette approche trace une analogie avec le m\'{e}canisme de cache de couches de Docker.

\paragraph{Hypoth\`{e}se.} Les mod\`{e}les de langage entra\^{i}n\'{e}s avec une couche de base forte en connaissances g\'{e}n\'{e}rales (phase intensive MMLU) exhiberont :
\begin{enumerate}
    \item Un apprentissage plus rapide sur les domaines sp\'{e}cialis\'{e}s que les lignes de base entra\^{i}n\'{e}es uniform\'{e}ment
    \item Un oubli catastrophique r\'{e}duit lorsque MMLU est maintenu \`{a} 5--10\% pendant le SFT sp\'{e}cifique au domaine
    \item Des scores MMLU stables comme indicateur d'une int\'{e}gration r\'{e}ussie des connaissances
\end{enumerate}

\paragraph{Lien avec l'architecture.} L'architecture COMPLEXITY-DEEP pourrait \^{e}tre particuli\`{e}rement adapt\'{e}e \`{a} cette strat\'{e}gie gr\^{a}ce \`{a} l'orthogonalisation des experts, la convergence du guidage $\mu$ et le scaling adaptatif.

\subsection{\'{E}tude d'ablation \`{a} l'inf\'{e}rence}
\label{sec:ablations}

Pour \'{e}valuer la contribution de chaque composant architectural, nous r\'{e}alisons des ablations \`{a} l'inf\'{e}rence sur le mod\`{e}le BASE pr\'{e}-entra\^{i}n\'{e}. Chaque composant est individuellement d\'{e}sactiv\'{e} en mettant \`{a} z\'{e}ro ses poids correspondants, tout en conservant les autres composants intacts. Nous \'{e}valuons sur 200 \'{e}chantillons par benchmark.

\begin{table}[ht]
\centering
\begin{tabular}{lccccc|c}
\toprule
\textbf{Configuration} & \textbf{MMLU} & \textbf{HellaS.} & \textbf{ARC-C} & \textbf{ARC-E} & \textbf{Wino.} & \textbf{Moy.} \\
\midrule
Mod\`{e}le complet & 21,5 & 24,5 & 22,0 & 32,0 & 48,5 & 29,7 \\
Sans Mu-Guidance & 22,5 & 25,5 & 21,0 & \textbf{24,5} & 52,5 & 29,2 \\
Sans Token-Routing & 21,0 & 25,5 & 24,0 & 31,5 & 47,0 & 29,8 \\
Sans PiD Controller & 25,0 & 27,5 & 24,5 & 29,5 & \textbf{45,0} & 30,3 \\
\midrule
\multicolumn{7}{l}{\textit{$\Delta$ vs Mod\`{e}le complet :}} \\
Sans Mu-Guidance & +1,0 & +1,0 & $-$1,0 & $-$7,5 & +4,0 & $-$0,5 \\
Sans Token-Routing & $-$0,5 & +1,0 & +2,0 & $-$0,5 & $-$1,5 & +0,1 \\
Sans PiD Controller & +3,5 & +3,0 & +2,5 & $-$2,5 & $-$3,5 & +0,6 \\
\bottomrule
\end{tabular}
\caption{R\'{e}sultats des ablations \`{a} l'inf\'{e}rence (\%). Les valeurs en gras indiquent des baisses notables lorsqu'un composant est retir\'{e}.}
\label{tab:ablations}
\end{table}

\textbf{Analyse.} Plusieurs observations \'{e}mergent de ces ablations \`{a} l'inf\'{e}rence :

\begin{enumerate}
    \item \textbf{Impact du Mu-Guidance sur le raisonnement} : la d\'{e}sactivation du Mu-Guidance provoque la plus forte baisse sur un benchmark unique : ARC-Easy chute de $-$7,5\%, sugg\'{e}rant que le flux d'information bidirectionnel est le plus b\'{e}n\'{e}fique pour les t\^{a}ches de raisonnement scientifique o\`{u} le contexte descendant aide \`{a} la s\'{e}lection de r\'{e}ponse.

    \item \textbf{Robustesse du Token-Routing} : l'impact minimal de la d\'{e}sactivation du Token-Routing \`{a} l'inf\'{e}rence est \textit{attendu} : puisque les poids du mod\`{e}le ont \'{e}t\'{e} optimis\'{e}s sous le routage modulo, les poids des experts encodent d\'{e}j\`{a} des sp\'{e}cialisations sp\'{e}cifiques au routage. La vraie contribution du Token-Routing est architecturale (r\'{e}duction du calcul par un facteur $n$, voir Th\'{e}or\`{e}me~\ref{thm:capacity}) plut\^{o}t que mesurable par une ablation \`{a} l'inf\'{e}rence.

    \item \textbf{PiD Controller comme stabilisateur d'entra\^{i}nement} : le r\^{o}le principal du contr\^{o}leur PiD est pendant l'entra\^{i}nement (stabilit\'{e} des gradients, convergence), pas l'inf\'{e}rence. Sa suppression \`{a} l'inf\'{e}rence permet des dynamiques d'activation l\'{e}g\`{e}rement diff\'{e}rentes qui peuvent am\'{e}liorer certains benchmarks de mani\`{e}re fortuite tout en d\'{e}gradant d'autres (Winogrande : $-$3,5\%).

    \item \textbf{Note m\'{e}thodologique} : il s'agit d'ablations \`{a} l'inf\'{e}rence sur un mod\`{e}le entra\^{i}n\'{e} avec tous les composants actifs. Cette m\'{e}thodologie fournit une \textit{borne inf\'{e}rieure} de l'importance des composants, car les poids co-s'adaptent pendant l'entra\^{i}nement. Des ablations \`{a} l'entra\^{i}nement compl\`{e}tes (r\'{e}-entra\^{i}nement from scratch sans chaque composant) fourniraient des preuves plus d\'{e}finitives et sont planifi\'{e}es comme travaux futurs.
\end{enumerate}

\subsection{Limitations et travaux futurs}
\label{sec:limitations}

\begin{itemize}
    \item \textbf{Comparaison \`{a} calcul \'{e}gal} : une comparaison rigoureuse avec Pythia ou d'autres lignes de base au m\^{e}me budget de tokens (33B) quantifierait l'efficacit\'{e} relative de l'architecture
    \item \textbf{Ablations \`{a} l'entra\^{i}nement} : bien que nos ablations \`{a} l'inf\'{e}rence (Section~\ref{sec:ablations}) fournissent des preuves initiales, des ablations compl\`{e}tes \`{a} l'entra\^{i}nement (r\'{e}-entra\^{i}nement from scratch sans chaque composant) sont n\'{e}cessaires pour quantifier d\'{e}finitivement la contribution de chaque innovation
    \item \textbf{Apprentissage continu} : valider l'hypoth\`{e}se MMLU-comme-d\'{e}pendance en mesurant les taux d'oubli lors de l'ajout de datasets sp\'{e}cialis\'{e}s (code, outils) avec et sans r\'{e}gularisation MMLU
    \item \textbf{Th\'{e}orie \'{e}tendue} : bien que nous \'{e}tablissions l'\'{e}quivalence de capacit\'{e} et l'orthogonalisation par le gradient (Th\'{e}or\`{e}mes~\ref{thm:capacity}, \ref{thm:gradient_orthogonalization}), une analyse compl\`{e}te des taux de convergence et de l'interaction entre le guidage Mu et le Token-Routing reste ouverte
    \item \textbf{Hypoth\`{e}se de d\'{e}pendance noyau} : la strat\'{e}gie d'ancrage MMLU (Section~\ref{sec:mmlu_anchor}) reste exploratoire et n\'{e}cessite une validation contr\^{o}l\'{e}e
\end{itemize}

%=============================================================================
\section{Conclusion}
\label{sec:conclusion}
%=============================================================================

Nous avons pr\'{e}sent\'{e} COMPLEXITY-DEEP, une architecture LLM novatrice introduisant trois contributions : le Token-Routed MLP pour un routage simplifi\'{e}, le Mu-Guided Attention pour un flux d'information bidirectionnel, et le Dynamic Scaler de style PiD pour la stabilit\'{e} de l'entra\^{i}nement. Nous avons fourni un fondement th\'{e}orique \`{a} travers des preuves formelles de l'\'{e}quilibre parfait de charge (Th\'{e}or\`{e}me~\ref{thm:perfect_balance}), de l'\'{e}quivalence de capacit\'{e} avec les mod\`{e}les denses (Th\'{e}or\`{e}me~\ref{thm:capacity}), et de l'orthogonalisation des experts guid\'{e}e par le gradient (Th\'{e}or\`{e}me~\ref{thm:gradient_orthogonalization}).

Notre impl\'{e}mentation \`{a} 1,5 milliard de param\`{e}tres, entra\^{i}n\'{e}e sur 33 milliards de tokens issus de FineWeb-Edu, d\'{e}montre la viabilit\'{e} de ces concepts avec une convergence stable (perte 3,78, perplexit\'{e} 43,7). L'\'{e}valuation sur des benchmarks standards montre des performances coh\'{e}rentes avec la taille du mod\`{e}le pour le mod\`{e}le BASE, avec une am\'{e}lioration substantielle apr\`{e}s fine-tuning supervis\'{e} (MMLU : 20,6\% $\to$ 30\%, ARC-Challenge : 17\% $\to$ 23\%).

Les \'{e}tudes d'ablation \`{a} l'inf\'{e}rence r\'{e}v\`{e}lent que le Mu-Guidance a l'impact le plus fort sur les benchmarks de raisonnement (ARC-Easy : $-$7,5\% lorsque d\'{e}sactiv\'{e}), tandis que les contributions du Token-Routing et du contr\^{o}leur PiD sont principalement architecturales (efficacit\'{e} computationnelle) et li\'{e}es \`{a} l'entra\^{i}nement (stabilit\'{e}), respectivement. Les travaux futurs incluront : (1) des ablations \`{a} l'entra\^{i}nement pour quantifier d\'{e}finitivement les contributions des composants, et (2) une validation rigoureuse de l'hypoth\`{e}se de d\'{e}pendance noyau \`{a} travers des exp\'{e}riences contr\^{o}l\'{e}es.

\subsubsection*{D\'{e}claration d'impact soci\'{e}tal}
Ce travail introduit des innovations architecturales pour les mod\`{e}les de langage. Bien que ces mod\`{e}les aient de nombreuses applications, ils comportent \'{e}galement des risques de mauvaise utilisation. Nous publions les poids du mod\`{e}le pour permettre la recherche tout en encourageant une utilisation responsable.

\subsubsection*{D\'{e}claration de reproductibilit\'{e}}
Pour faciliter la reproductibilit\'{e} et la r\'{e}vision, nous fournissons l'impl\'{e}mentation compl\`{e}te de l'architecture du mod\`{e}le en mat\'{e}riel suppl\'{e}mentaire (\texttt{supplementary\_code.zip}). Le code (PyTorch 2.0+) sera rendu publiquement disponible apr\`{e}s acceptation.

\bibliography{references}
\bibliographystyle{tmlr}

\appendix
\section{Courbes d'entra\^{i}nement}
\label{app:training}

Des courbes d'entra\^{i}nement et figures d'analyse suppl\'{e}mentaires sont disponibles dans le mat\'{e}riel suppl\'{e}mentaire.

\end{document}
